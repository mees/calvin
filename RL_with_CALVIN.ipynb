{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "RL_with_CALVIN.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNwSDlhfoXLR2ORP+8/+3zP",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mees/calvin/blob/main/RL_with_CALVIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDAC0SvBi1-9"
   },
   "source": [
    "<h1>Reinforcement Learning with CALVIN</h1>\n",
    "\n",
    "The **CALVIN** simulated benchmark is perfectly suited for training agents with reinforcement learning, in this notebook we will demonstrate how to integrate your agents to these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk7gEiqXizze"
   },
   "source": [
    "## Installation\n",
    "The first step is to install the CALVIN github repository such that we have access to the packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K1cSDZwLlAhR"
   },
   "source": [
    "# Download repo\n",
    "%mkdir /content/calvin\n",
    "%cd /content/calvin\n",
    "!git clone https://github.com/mees/calvin_env.git\n",
    "%cd /content/calvin/calvin_env\n",
    "!git clone https://github.com/lukashermann/tacto.git\n",
    "# Install packages \n",
    "%cd /content/calvin/calvin_env/tacto/\n",
    "!pip3 install -e .\n",
    "%cd /content/calvin/calvin_env\n",
    "!pip3 install -e .\n",
    "!pip3 install -U numpy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hPiKJ7mY58o3"
   },
   "source": [
    "# Run this to check if the installation was succesful\n",
    "from calvin_env.envs.play_table_env import PlayTableSimEnv"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSmDkjlx0FOW"
   },
   "source": [
    "## Loading the environment\n",
    "After the installation has finished successfully, we can start using the environment for reinforcement Learning.\n",
    "To be able to use the environment we need to have the appropriate configuration that define the desired features, for this example, we will load the static and gripper camera."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vp1-mgIgekvY"
   },
   "source": [
    "%cd /content/calvin\n",
    "from hydra import initialize, compose\n",
    "\n",
    "with initialize(config_path=\"./calvin_env/conf/\"):\n",
    "  cfg = compose(config_name=\"config_data_collection.yaml\", overrides=[\"cameras=static_and_gripper\"])\n",
    "  cfg.env[\"use_egl\"] = False\n",
    "  cfg.env[\"show_gui\"] = False\n",
    "  cfg.env[\"use_vr\"] = False\n",
    "  cfg.env[\"use_scene_info\"] = True\n",
    "  print(cfg.env)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onN9sssj1JV6"
   },
   "source": [
    "The environment has similar structure to traditional OpenAI Gym environments.\n",
    "\n",
    "*   We can restart the simulation with the *reset* function.\n",
    "*   We can perform an action in the environment with the *step* function.\n",
    "*   We can visualize images taken from the cameras in the environment by using the *render* function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MwIuxueazYOh"
   },
   "source": [
    "import time\n",
    "import hydra\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "env = hydra.utils.instantiate(cfg.env)\n",
    "observation = env.reset()\n",
    "#The observation is given as a dictionary with different values\n",
    "print(observation.keys())\n",
    "for i in range(5):\n",
    "  # The action consists in a pose displacement (position and orientation)\n",
    "  action_displacement = np.random.uniform(low=-1, high=1, size=6)\n",
    "  # And a binary gripper action, -1 for closing and 1 for oppening\n",
    "  action_gripper = np.random.choice([-1, 1], size=1)\n",
    "  action = np.concatenate((action_displacement, action_gripper), axis=-1)\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  rgb = env.render(mode=\"rgb_array\")[:,:,::-1]\n",
    "  cv2_imshow(rgb)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0yYw5pHC0eE"
   },
   "source": [
    "##  Custom environment for Reinforcement Learning\n",
    "There are some aspects that needs to be defined to be able to use it for reinforcement learning, including:\n",
    "\n",
    "1.   Observation space\n",
    "2.   Action space\n",
    "3.   Reward function\n",
    "\n",
    "We are going to create a Custom environment that extends the **PlaytableSimEnv** to add these requirements. <br/>\n",
    "The specific task that will be solved is called \"move_slider_left\", here you can find a [list of possible tasks](https://github.com/mees/calvin_env/blob/main/conf/tasks/new_playtable_tasks.yaml) that can be evaluated using CALVIN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bwj-5UQz2vyF"
   },
   "source": [
    "from gym import spaces\n",
    "from calvin_env.envs.play_table_env import PlayTableSimEnv\n",
    "\n",
    "class SlideEnv(PlayTableSimEnv):\n",
    "    def __init__(self,\n",
    "                 tasks: dict = {},\n",
    "                 **kwargs):\n",
    "        super(SlideEnv, self).__init__(**kwargs)\n",
    "        # For this example we will modify the observation to\n",
    "        # only retrieve the end effector pose\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(7,))\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(7,))\n",
    "        # We can use the task utility to know if the task was executed correctly\n",
    "        self.tasks = hydra.utils.instantiate(tasks)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = super().reset()\n",
    "        self.start_info = self.get_info()\n",
    "        return obs\n",
    "\n",
    "    def get_obs(self):\n",
    "        \"\"\"Overwrite robot obs to only retrieve end effector position\"\"\"\n",
    "        robot_obs, robot_info = self.robot.get_observation()\n",
    "        return robot_obs[:7]\n",
    "\n",
    "    def _success(self):\n",
    "        \"\"\" Returns a boolean indicating if the task was performed correctly \"\"\"\n",
    "        current_info = self.get_info()\n",
    "        task_filter = [\"move_slider_left\"]\n",
    "        task_info = self.tasks.get_task_info_for_set(self.start_info, current_info, task_filter)\n",
    "        return 'move_slider_left' in task_info\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\" Returns the reward function that will be used \n",
    "        for the RL algorithm \"\"\"\n",
    "        reward = int(self._success()) * 10\n",
    "        r_info = {'reward': reward}\n",
    "        return reward, r_info\n",
    "\n",
    "    def _termination(self):\n",
    "        \"\"\" Indicates if the robot has reached a terminal state \"\"\"\n",
    "        success = self._success()\n",
    "        done = success\n",
    "        d_info = {'success': success}        \n",
    "        return done, d_info\n",
    "\n",
    "    def step(self, action):\n",
    "            \"\"\" Performing a relative action in the environment\n",
    "                input:\n",
    "                    action: 7 tuple containing\n",
    "                            Position x, y, z. \n",
    "                            Angle in rad x, y, z. \n",
    "                            Gripper action\n",
    "                            each value in range (-1, 1)\n",
    "\n",
    "                            OR\n",
    "                            8 tuple containing\n",
    "                            Relative Joint angles j1 - j7 (in rad)\n",
    "                            Gripper action\n",
    "                output:\n",
    "                    observation, reward, done info\n",
    "            \"\"\"\n",
    "            # Transform gripper action to discrete space\n",
    "            env_action = action.copy()\n",
    "            env_action[-1] = (int(action[-1] >= 0) * 2) - 1\n",
    "\n",
    "            # for using actions in joint space\n",
    "            if len(env_action) == 8:\n",
    "                env_action = {\"action\": env_action, \"type\": \"joint_rel\"}\n",
    "\n",
    "            self.robot.apply_action(env_action)\n",
    "            for i in range(self.action_repeat):\n",
    "                self.p.stepSimulation(physicsClientId=self.cid)\n",
    "            obs = self.get_obs()\n",
    "            info = self.get_info()\n",
    "            reward, r_info = self._reward()\n",
    "            done, d_info = self._termination()\n",
    "            info.update(r_info)\n",
    "            info.update(d_info)\n",
    "            return obs, reward, done, info"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJo-HWRqcJHc"
   },
   "source": [
    "# Training an RL agent\n",
    "After generating the wrapper training a reinforcement learning agent is straightforward, for this example we will use stable baselines 3 agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XOkD9S-iMVcj"
   },
   "source": [
    "!pip3 install stable_baselines3"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxWRcJJFcpEF"
   },
   "source": [
    "To train the agent we create an instance of our new environment and send it to the stable baselines agent to learn a policy.\n",
    "\n",
    "\n",
    "> Note: the example uses Soft Actor Critic (SAC) which is one of the state of the art algorithm for off-policy RL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3BUeaAnqMNLq"
   },
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "new_env_cfg = {**cfg.env}\n",
    "new_env_cfg[\"tasks\"] = cfg.tasks\n",
    "new_env_cfg.pop('_target_', None)\n",
    "new_env_cfg.pop('_recursive_', None)\n",
    "env = SlideEnv(**new_env_cfg)\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
